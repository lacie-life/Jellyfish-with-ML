{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Anomalies using Heterogeneous GNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, anomaly detection is a popular task that aims to identify patterns or observations\n",
    "in data that deviate from the expected behavior. This is a fundamental problem that arises in many\n",
    "real-world applications, such as detecting fraud in financial transactions, identifying defective products\n",
    "in a manufacturing process, and detecting cyber attacks in a computer network.\n",
    "\n",
    "GNNs can be trained to learn the normal behavior of a network and then identify nodes or patterns\n",
    "that deviate from that behavior. Indeed, their ability to understand complex relationships makes them\n",
    "particularly appropriate to detect weak signals. Additionally, GNNs can be scaled to large datasets,\n",
    "making them an efficient tool for processing large amounts of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the CIDDS-001 dataset\n",
    "\n",
    "The CIDDS-001 dataset is designed to train and evaluate anomaly-based network intrusion\n",
    "detection systems. It provides realistic traffic that includes up-to-date attacks to assess these systems. It\n",
    "was created by collecting and labeling 8,451,520 traffic flows in a virtual environment using OpenStack.\n",
    "Precisely, each row corresponds to a NetFlow connection, describing Internet Protocol (IP) traffic\n",
    "statistics, such as the number of bytes exchanged.\n",
    "\n",
    "The following figure provides an overview of the simulated network environment in CIDDS-001.\n",
    "\n",
    "![img](./1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see four different subnets (developer, office, management, and server) with their respective IP\n",
    "address ranges. All these subnets are linked to a single server connected to the internet through a\n",
    "firewall. An external server is also present and provides two services: a file synchronization service\n",
    "and a web server. Finally, attackers are represented outside of the local network.\n",
    "\n",
    "Connections in CIDDS-001 were collected from the local and external servers. The goal of this\n",
    "dataset is to correctly classify these connections into five categories: benign (no attack), brute-force,\n",
    "denial of service, ping scan, and port scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "!pip install -q torch-scatter~=2.1.0 torch-sparse~=0.6.16 torch-cluster~=1.6.0 torch-spline-conv~=1.2.1 torch-geometric==2.2.0 -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "url = 'https://www.hs-coburg.de/fileadmin/hscoburg/WISENT-CIDDS-001.zip'\n",
    "with urlopen(url) as zurl:\n",
    "    with ZipFile(BytesIO(zurl.read())) as zfile:\n",
    "        zfile.extractall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"CIDDS-001/traffic/OpenStack/CIDDS-001-internal-week1.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few interesting features we can use for our model:\n",
    "\n",
    "• The date first seen is a timestamp we can process to extract information about the day of the\n",
    "week and the time of day. In general, network traffic is seasonal, and connections that occur\n",
    "at night or on unusual days are suspicious.\n",
    "\n",
    "• IP addresses (such as 192.168.100.5) are notoriously difficult to process because they are\n",
    "not numerical values and follow a complex set of rules. We could bin them into a few categories\n",
    "since we know how our local network is set up. Another popular and more generalizable solution\n",
    "is to convert them into a binary representation (“192” becomes “11000000”).\n",
    "\n",
    "• Duration, the number of packets, and the number of bytes are features that usually display\n",
    "heavy-tailed distributions. Therefore, they will require special processing if that is the case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Src Pt', 'Dst Pt', 'Flows', 'Tos', 'class', 'attackID', 'attackDescription'])\n",
    "df['attackType'] = df['attackType'].replace('---', 'benign')\n",
    "df['Date first seen'] = pd.to_datetime(df['Date first seen'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_labels = df['attackType'].value_counts() / len(df) * 100\n",
    "print(count_labels)\n",
    "plt.pie(count_labels[:3], labels=df['attackType'].unique()[:3], autopct='%.0f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2, ax3)) = plt.subplots(1, 3, figsize=(15,5))\n",
    "df['Duration'].hist(ax=ax1)\n",
    "ax1.set_xlabel(\"Duration\")\n",
    "df['Packets'].hist(ax=ax2)\n",
    "ax2.set_xlabel(\"Number of packets\")\n",
    "pd.to_numeric(df['Bytes'], errors='coerce').hist(ax=ax3)\n",
    "ax3.set_xlabel(\"Number of bytes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weekday'] = df['Date first seen'].dt.weekday\n",
    "df = pd.get_dummies(df, columns=['weekday']).rename(columns = {'weekday_0': 'Monday',\n",
    "                                                              'weekday_1': 'Tuesday',\n",
    "                                                              'weekday_2': 'Wednesday',\n",
    "                                                              'weekday_3': 'Thursday',\n",
    "                                                              'weekday_4': 'Friday',\n",
    "                                                              'weekday_5': 'Saturday',\n",
    "                                                              'weekday_6': 'Sunday',\n",
    "                                                             })\n",
    "                                                             \n",
    "df['daytime'] = (df['Date first seen'].dt.second +df['Date first seen'].dt.minute*60 + df['Date first seen'].dt.hour*60*60)/(24*60*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_flags(input):\n",
    "    return [1 if char1 == char2 else 0 for char1, char2 in zip('APRSF', input[1:])]\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "ohe_flags = one_hot_flags(df['Flags'].to_numpy())\n",
    "ohe_flags = df['Flags'].apply(one_hot_flags).to_list()\n",
    "df[['ACK', 'PSH', 'RST', 'SYN', 'FIN']] = pd.DataFrame(ohe_flags, columns=['ACK', 'PSH', 'RST', 'SYN', 'FIN'])\n",
    "df = df.drop(columns=['Date first seen', 'Flags'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame()\n",
    "temp['SrcIP'] = df['Src IP Addr'].astype(str)\n",
    "temp['SrcIP'][~temp['SrcIP'].str.contains('\\d{1,3}\\.', regex=True)] = '0.0.0.0'\n",
    "temp = temp['SrcIP'].str.split('.', expand=True).rename(columns = {2: 'ipsrc3', 3: 'ipsrc4'}).astype(int)[['ipsrc3', 'ipsrc4']]\n",
    "temp['ipsrc'] = temp['ipsrc3'].apply(lambda x: format(x, \"b\").zfill(8)) + temp['ipsrc4'].apply(lambda x: format(x, \"b\").zfill(8))\n",
    "df = df.join(temp['ipsrc'].str.split('', expand=True)\n",
    "            .drop(columns=[0, 17])\n",
    "            .rename(columns=dict(enumerate([f'ipsrc_{i}' for i in range(17)])))\n",
    "            .astype('int32'))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame()\n",
    "temp['DstIP'] = df['Dst IP Addr'].astype(str)\n",
    "temp['DstIP'][~temp['DstIP'].str.contains('\\d{1,3}\\.', regex=True)] = '0.0.0.0'\n",
    "temp = temp['DstIP'].str.split('.', expand=True).rename(columns = {2: 'ipdst3', 3: 'ipdst4'}).astype(int)[['ipdst3', 'ipdst4']]\n",
    "temp['ipdst'] = temp['ipdst3'].apply(lambda x: format(x, \"b\").zfill(8)) \\\n",
    "                + temp['ipdst4'].apply(lambda x: format(x, \"b\").zfill(8))\n",
    "df = df.join(temp['ipdst'].str.split('', expand=True)\n",
    "            .drop(columns=[0, 17])\n",
    "            .rename(columns=dict(enumerate([f'ipdst_{i}' for i in range(17)])))\n",
    "            .astype('int32'))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_index = df[pd.to_numeric(df['Bytes'], errors='coerce').isnull() == True].index\n",
    "df['Bytes'].loc[m_index] = df['Bytes'].loc[m_index].apply(lambda x: 10e6 * float(x.strip().split()[0]))\n",
    "df['Bytes'] = pd.to_numeric(df['Bytes'], errors='coerce', downcast='integer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['benign', 'bruteForce', 'dos', 'pingScan', 'portScan']\n",
    "df_train, df_test = train_test_split(df, random_state=0, test_size=0.2, stratify=df[labels])\n",
    "df_val, df_test = train_test_split(df_test, random_state=0, test_size=0.5, stratify=df_test[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = PowerTransformer()\n",
    "df_train[['Duration', 'Packets', 'Bytes']] = scaler.fit_transform(df_train[['Duration', 'Packets', 'Bytes']])\n",
    "df_val[['Duration', 'Packets', 'Bytes']] = scaler.transform(df_val[['Duration', 'Packets', 'Bytes']])\n",
    "df_test[['Duration', 'Packets', 'Bytes']] = scaler.transform(df_test[['Duration', 'Packets', 'Bytes']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['benign'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2, ax3)) = plt.subplots(1, 3, figsize=(15,5))\n",
    "df_train['Duration'].hist(ax=ax1)\n",
    "ax1.set_xlabel(\"Duration\")\n",
    "df_train['Packets'].hist(ax=ax2)\n",
    "ax2.set_xlabel(\"Number of packets\")\n",
    "df_train['Bytes'].hist(ax=ax3)\n",
    "ax3.set_xlabel(\"Number of bytes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a heterogeneous GNN\n",
    "\n",
    "These new distributions are not Gaussian, but the values are more spread out, which should help\n",
    "the model.\n",
    "\n",
    "Note that the dataset we processed is purely tabular. We still need to convert it into a graph dataset\n",
    "before we can feed it to a GNN. In our case, there is no obvious way of converting our traffic flows\n",
    "into nodes. Ideally, flows between the same computers should be connected. This can be achieved\n",
    "using a heterogeneous graph with two types of nodes:\n",
    "\n",
    "• Hosts, which correspond to computers and use IP addresses as features. If we had more\n",
    "information, we could add other computer-related features, such as logs or CPU utilization.\n",
    "\n",
    "• Flows, which correspond to connections between two hosts. They consider all the other features\n",
    "from the dataset. They also have the label we want to predict (a benign or malicious flow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "features_host = [f'ipsrc_{i}' for i in range(1, 17)] + [f'ipdst_{i}' for i in range(1, 17)]\n",
    "features_flow = ['daytime', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Duration', 'Packets', 'Bytes', 'ACK', 'PSH', 'RST', 'SYN', 'FIN', 'ICMP ', 'IGMP ', 'TCP  ', 'UDP  ']\n",
    "\n",
    "def get_connections(ip_map, src_ip, dst_ip):\n",
    "    src1 = [ip_map[ip] for ip in src_ip]\n",
    "    src2 = [ip_map[ip] for ip in dst_ip]\n",
    "    src = np.column_stack((src1, src2)).flatten()\n",
    "    dst = list(range(len(src_ip)))\n",
    "    dst = np.column_stack((dst, dst)).flatten()\n",
    "    \n",
    "    return torch.Tensor([src, dst]).int(), torch.Tensor([dst, src]).int()\n",
    "\n",
    "def create_dataloader(df, subgraph_size=1024):\n",
    "    data = []\n",
    "    n_subgraphs = len(df) // subgraph_size\n",
    "    for i in range(1, n_subgraphs+1):\n",
    "        subgraph = df[(i-1)*subgraph_size:i*subgraph_size]\n",
    "        src_ip = subgraph['Src IP Addr'].to_numpy()\n",
    "        dst_ip = subgraph['Dst IP Addr'].to_numpy()\n",
    "        \n",
    "        ip_map = {ip:index for index, ip in enumerate(np.unique(np.append(src_ip, dst_ip)))}\n",
    "        host_to_flow, flow_to_host = get_connections(ip_map, src_ip, dst_ip)\n",
    "\n",
    "        batch = HeteroData()\n",
    "        batch['host'].x = torch.Tensor(subgraph[features_host].to_numpy()).float()\n",
    "        batch['flow'].x = torch.Tensor(subgraph[features_flow].to_numpy()).float()\n",
    "        batch['flow'].y = torch.Tensor(subgraph[labels].to_numpy()).float()\n",
    "        batch['host','flow'].edge_index = host_to_flow\n",
    "        batch['flow','host'].edge_index = flow_to_host\n",
    "        data.append(batch)\n",
    "\n",
    "    return DataLoader(data, batch_size=BATCH_SIZE)\n",
    "\n",
    "train_loader = create_dataloader(df_train)\n",
    "val_loader = create_dataloader(df_val)\n",
    "test_loader = create_dataloader(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch_geometric.nn import Linear, HeteroConv, SAGEConv, GATConv\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, dim_h, dim_out, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HeteroConv({\n",
    "                ('host', 'to', 'flow'): SAGEConv((-1,-1), dim_h, add_self_loops=False),\n",
    "                ('flow', 'to', 'host'): SAGEConv((-1,-1), dim_h, add_self_loops=False),\n",
    "            }, aggr='sum')\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.lin = Linear(dim_h, dim_out)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            x_dict = {key: F.leaky_relu(x) for key, x in x_dict.items()}\n",
    "        return self.lin(x_dict['flow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HeteroGNN(dim_h=64, dim_out=5, num_layers=3).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    n_subgraphs = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch.to(device)\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)\n",
    "        loss = F.cross_entropy(out, batch['flow'].y.float())\n",
    "        y_pred.append(out.argmax(dim=1))\n",
    "        y_true.append(batch['flow'].y.argmax(dim=1))\n",
    "        n_subgraphs += BATCH_SIZE\n",
    "        total_loss += float(loss) * BATCH_SIZE\n",
    "        \n",
    "    y_pred = torch.cat(y_pred).cpu()\n",
    "    y_true = torch.cat(y_true).cpu()\n",
    "    f1score = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    return total_loss/n_subgraphs, f1score, y_pred, y_true\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(101):\n",
    "    n_subgraphs = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch.to(device)\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)\n",
    "        loss = F.cross_entropy(out, batch['flow'].y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        n_subgraphs += BATCH_SIZE\n",
    "        total_loss += float(loss) * BATCH_SIZE\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        val_loss, f1score, _, _ = test(val_loader)\n",
    "        print(f'Epoch {epoch} | Loss: {total_loss/n_subgraphs:.4f} | Val loss: {val_loss:.4f} | Val F1-score: {f1score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, y_pred, y_true = test(test_loader)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_pred = pd.DataFrame([y_pred.numpy(), y_true.numpy()]).T\n",
    "df_pred.columns = ['pred', 'true']\n",
    "plt.pie(df_pred['true'][df_pred['pred'] != df_pred['true']].value_counts(), labels=labels, autopct='%.0f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "norm_matrix = matrix / matrix.sum(axis=1) * 100\n",
    "\n",
    "plt.imshow(norm_matrix, cmap='Blues')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.xticks(range(len(labels)), labels)\n",
    "plt.yticks(range(len(labels)), labels)\n",
    "    \n",
    "for i, j in itertools.product(range(matrix.shape[0]), range(matrix.shape[1])):\n",
    "    text = f\"{matrix[i,j]:,}\\n{norm_matrix[i,j]:.2f}%\"\n",
    "    plt.text(j, i, text,\n",
    "            horizontalalignment='center', verticalalignment='center',\n",
    "            color='white' if matrix[i,j] >= matrix[i,:].mean() else 'black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
