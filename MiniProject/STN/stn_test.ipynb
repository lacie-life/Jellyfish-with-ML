{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lacie/miniconda3/envs/jis/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "        # Spatial transformer localization-network\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        # Regressor for the 3x2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "                nn.Linear(10 * 3 * 3, 32),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(32, 3*2)\n",
    "                )\n",
    "        \n",
    "        # Initialize the weights/bias with identity transformation \n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1,0,0,0,1,0], dtype=torch.float))\n",
    "            \n",
    "    def affine_grid(self, theta, size, align_corners=False):\n",
    "        N, C, H, W = size\n",
    "        grid = self.create_grid(N, C, H, W).to(theta.device)\n",
    "        grid = grid.view(N, H * W, 3).bmm(theta.transpose(1, 2))\n",
    "        grid = grid.view(N, H, W, 2)\n",
    "        return grid\n",
    "\n",
    "    def create_grid(self, N, C, H, W):\n",
    "        grid = torch.empty((N, H, W, 3), dtype=torch.float32)\n",
    "        grid.select(-1, 0).copy_(self.linspace_from_neg_one(W))\n",
    "        grid.select(-1, 1).copy_(self.linspace_from_neg_one(H).unsqueeze_(-1))\n",
    "        grid.select(-1, 2).fill_(1)\n",
    "        return grid\n",
    "    \n",
    "    def linspace_from_neg_one(self, num_steps, dtype=torch.float32):\n",
    "        r = torch.linspace(-1, 1, num_steps, dtype=torch.float32)\n",
    "        r = r * (num_steps - 1) / num_steps\n",
    "        return r\n",
    "    \n",
    "    def grid_sample(self, im, grid, align_corners=False):\n",
    "        # https://github.com/open-mmlab/mmcv/blob/master/mmcv/ops/point_sample.py\n",
    "        \n",
    "        n, c, h, w = im.shape\n",
    "        gn, gh, gw, _ = grid.shape\n",
    "        # assert n == gn\n",
    "\n",
    "        x = grid[:, :, :, 0]\n",
    "        y = grid[:, :, :, 1]\n",
    "\n",
    "        if align_corners:\n",
    "            x = ((x + 1) / 2) * (w - 1)\n",
    "            y = ((y + 1) / 2) * (h - 1)\n",
    "        else:\n",
    "            x = ((x + 1) * w - 1) / 2\n",
    "            y = ((y + 1) * h - 1) / 2\n",
    "\n",
    "        x = x.view(n, -1)\n",
    "        y = y.view(n, -1)\n",
    "\n",
    "        x0 = torch.floor(x).long()\n",
    "        y0 = torch.floor(y).long()\n",
    "        x1 = x0 + 1\n",
    "        y1 = y0 + 1\n",
    "\n",
    "        wa = ((x1 - x) * (y1 - y)).unsqueeze(1)\n",
    "        wb = ((x1 - x) * (y - y0)).unsqueeze(1)\n",
    "        wc = ((x - x0) * (y1 - y)).unsqueeze(1)\n",
    "        wd = ((x - x0) * (y - y0)).unsqueeze(1)\n",
    "\n",
    "        # Apply default for grid_sample function zero padding\n",
    "        im_padded = F.pad(im, pad=[1, 1, 1, 1], mode='constant', value=0)\n",
    "        padded_h = h + 2\n",
    "        padded_w = w + 2\n",
    "        \n",
    "        # save points positions after padding\n",
    "        x0, x1, y0, y1 = x0 + 1, x1 + 1, y0 + 1, y1 + 1\n",
    "\n",
    "        # Clip coordinates to padded image size\n",
    "        zero = torch.tensor(0).to(x0.device)\n",
    "        \n",
    "        x0 = torch.where(x0 < 0, zero, x0)\n",
    "        x0 = torch.where(x0 > padded_w - 1, torch.tensor(padded_w - 1).to(x0.device), x0)\n",
    "        \n",
    "        x1 = torch.where(x1 < 0, zero, x1)\n",
    "        x1 = torch.where(x1 > padded_w - 1, torch.tensor(padded_w - 1).to(x0.device), x1)\n",
    "        \n",
    "        y0 = torch.where(y0 < 0, zero, y0)\n",
    "        y0 = torch.where(y0 > padded_h - 1, torch.tensor(padded_h - 1).to(x0.device), y0)\n",
    "        \n",
    "        y1 = torch.where(y1 < 0, zero, y1)\n",
    "        y1 = torch.where(y1 > padded_h - 1, torch.tensor(padded_h - 1).to(x0.device), y1)\n",
    "\n",
    "        im_padded = im_padded.view(n, c, -1)\n",
    "\n",
    "        x0_y0 = (x0 + y0 * padded_w).unsqueeze(1).expand(-1, c, -1)\n",
    "        x0_y1 = (x0 + y1 * padded_w).unsqueeze(1).expand(-1, c, -1)\n",
    "        x1_y0 = (x1 + y0 * padded_w).unsqueeze(1).expand(-1, c, -1)\n",
    "        x1_y1 = (x1 + y1 * padded_w).unsqueeze(1).expand(-1, c, -1)\n",
    "\n",
    "        Ia = torch.gather(im_padded, 2, x0_y0)\n",
    "        Ib = torch.gather(im_padded, 2, x0_y1)\n",
    "        Ic = torch.gather(im_padded, 2, x1_y0)\n",
    "        Id = torch.gather(im_padded, 2, x1_y1)\n",
    "\n",
    "        return (Ia * wa + Ib * wb + Ic * wc + Id * wd).reshape(n, c, gh, gw)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        \n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * 3 * 3)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1,2,3)\n",
    "        \n",
    "        # grid = F.affine_grid(theta, (N, C, H, W))\n",
    "        grid = self.affine_grid(theta, (N, C, H, W))\n",
    "\n",
    "        # x = torch.nn.functional.grid_sample(x, grid, align_corners=False)\n",
    "        x = self.grid_sample(x, grid, align_corners=False)\n",
    "        \n",
    "        # Perform the usual forward pass\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "model = Net().to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [00:16, 112.43it/s]\n",
      "1875it [00:16, 114.45it/s]\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "# Training loop\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in tqdm(enumerate(train_loader)):\n",
    "        data, target = data.to('cuda:0'), target.to('cuda:0')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        # print(output.shape)\n",
    "        # print(target.shape)\n",
    "\n",
    "        loss = torch.nn.functional.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7886/1717967565.py:89: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  zero = torch.tensor(0).to(x0.device)\n",
      "/tmp/ipykernel_7886/1717967565.py:92: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  x0 = torch.where(x0 > padded_w - 1, torch.tensor(padded_w - 1).to(x0.device), x0)\n",
      "/tmp/ipykernel_7886/1717967565.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x0 = torch.where(x0 > padded_w - 1, torch.tensor(padded_w - 1).to(x0.device), x0)\n",
      "/tmp/ipykernel_7886/1717967565.py:95: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  x1 = torch.where(x1 > padded_w - 1, torch.tensor(padded_w - 1).to(x0.device), x1)\n",
      "/tmp/ipykernel_7886/1717967565.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x1 = torch.where(x1 > padded_w - 1, torch.tensor(padded_w - 1).to(x0.device), x1)\n",
      "/tmp/ipykernel_7886/1717967565.py:98: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  y0 = torch.where(y0 > padded_h - 1, torch.tensor(padded_h - 1).to(x0.device), y0)\n",
      "/tmp/ipykernel_7886/1717967565.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y0 = torch.where(y0 > padded_h - 1, torch.tensor(padded_h - 1).to(x0.device), y0)\n",
      "/tmp/ipykernel_7886/1717967565.py:101: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  y1 = torch.where(y1 > padded_h - 1, torch.tensor(padded_h - 1).to(x0.device), y1)\n",
      "/tmp/ipykernel_7886/1717967565.py:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y1 = torch.where(y1 > padded_h - 1, torch.tensor(padded_h - 1).to(x0.device), y1)\n",
      "/home/lacie/miniconda3/envs/jis/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/home/lacie/miniconda3/envs/jis/lib/python3.8/site-packages/torch/onnx/utils.py:687: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/home/lacie/miniconda3/envs/jis/lib/python3.8/site-packages/torch/onnx/utils.py:1178: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "# Export the model to ONNX format\n",
    "dummy_input = torch.randn(1, 1, 28, 28)  # Example input shape\n",
    "torch.onnx.export(model, dummy_input.to('cuda:0'), \"stn_mnist.onnx\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
