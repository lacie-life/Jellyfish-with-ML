{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(edgeitems=2, threshold=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to the shape of tensors, video data can be seen as equivalent to volumetric data, with `depth` replaced by the `time` dimension. The result is again a 5D tensor with shape `N x C x T x H x W`.\n",
    "\n",
    "There are several formats for video, especially geared towards compression by exploiting redundancies in space and time. Luckily for us, `imageio` reads video data as well. Suppose we'd like to retain 100 consecutive frames in our 512 x 512 RBG video for classifying an action using a convolutional neural network. We first create a reader instance for the video, that will allow us to get information about the video and iterate over the frames in time.\n",
    "Let's see what the meta data for the video looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lacie/miniconda3/envs/conda-pytorch/lib/python3.8/site-packages/imageio/plugins/pyav.py:731: UserWarning: PyAV 10.0.0 has known issues reading metadata. If you need video metadata consider using v9.2.0 instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'Fraction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mimageio\u001b[39;00m\n\u001b[1;32m      3\u001b[0m reader \u001b[39m=\u001b[39m imageio\u001b[39m.\u001b[39mget_reader(\u001b[39m'\u001b[39m\u001b[39m../../data/p1ch4/video-cockatoo/cockatoo.mp4\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m meta \u001b[39m=\u001b[39m reader\u001b[39m.\u001b[39;49mget_meta_data()\n\u001b[1;32m      5\u001b[0m meta\n",
      "File \u001b[0;32m~/miniconda3/envs/conda-pytorch/lib/python3.8/site-packages/imageio/v2.py:162\u001b[0m, in \u001b[0;36mLegacyReader.get_meta_data\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_meta_data\u001b[39m(\u001b[39mself\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minstance\u001b[39m.\u001b[39;49mmetadata(index\u001b[39m=\u001b[39;49mindex, exclude_applied\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/conda-pytorch/lib/python3.8/site-packages/imageio/plugins/pyav.py:763\u001b[0m, in \u001b[0;36mPyAVPlugin.metadata\u001b[0;34m(self, index, exclude_applied, constant_framerate)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[39mif\u001b[39;00m constant_framerate \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    761\u001b[0m     constant_framerate \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container\u001b[39m.\u001b[39mformat\u001b[39m.\u001b[39mvariable_fps\n\u001b[0;32m--> 763\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_seek(index, constant_framerate\u001b[39m=\u001b[39;49mconstant_framerate)\n\u001b[1;32m    764\u001b[0m desired_frame \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoder)\n\u001b[1;32m    765\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/conda-pytorch/lib/python3.8/site-packages/imageio/plugins/pyav.py:1154\u001b[0m, in \u001b[0;36mPyAVPlugin._seek\u001b[0;34m(self, index, constant_framerate)\u001b[0m\n\u001b[1;32m   1151\u001b[0m sec_delta \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_video_stream\u001b[39m.\u001b[39mguessed_rate\n\u001b[1;32m   1152\u001b[0m pts_delta \u001b[39m=\u001b[39m sec_delta \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_video_stream\u001b[39m.\u001b[39mtime_base\n\u001b[0;32m-> 1154\u001b[0m index_pts \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(index \u001b[39m*\u001b[39;49m pts_delta)\n\u001b[1;32m   1156\u001b[0m \u001b[39m# this only seeks to the closed (preceeding) keyframe\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container\u001b[39m.\u001b[39mseek(index_pts, stream\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_video_stream)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'Fraction'"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "\n",
    "reader = imageio.get_reader('../../data/p1ch4/video-cockatoo/cockatoo.mp4')\n",
    "meta = reader.get_meta_data()\n",
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the information to size the tensor that will store the video frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 280, 1280, 720])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_channels = 3\n",
    "n_frames = meta['nframes']\n",
    "video = torch.empty(n_channels, n_frames, *meta['size'])\n",
    "\n",
    "video.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just iterate over the reader and set the values for all three channels into in the proper `i`-th time slice.\n",
    "This might take a few seconds to finish!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, frame_arr in enumerate(reader):\n",
    "    frame = torch.from_numpy(frame_arr).float()\n",
    "    video[:, i] = torch.transpose(frame, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we iterate over individual frames and set each frame in the `C x T x H x W` video tensor, after transposing the channel. We can then obtain a batch by stacking multiple 4D tensors or pre-allocating a 5D tensor with a known batch size and filling it iteratively, clip by clip, assuming clips are trimmed to a fixed number of frames.\n",
    "\n",
    "Equating video data to volumetric data is not the only way to represent video for training purposes. This is a valid strategy if we deal with video bursts of fixed length. An alternative strategy is to resort to network architectures capable of processing long sequences and exploiting short and long-term relationships in time, just like for text or audio.\n",
    "// We'll see this kind of architectures when we take on recurrent networks.\n",
    "\n",
    "This next approach accounts for time along the batch dimension. Hence, we'll build our dataset as a 4D tensor, stacking frame by frame in the batch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([280, 3, 1280, 720])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_video = torch.empty(n_frames, n_channels, *meta['size'])\n",
    "\n",
    "for i, frame in enumerate(reader):\n",
    "    frame = torch.from_numpy(frame).float()\n",
    "    time_video[i] = torch.transpose(frame, 0, 2)\n",
    "\n",
    "time_video.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
